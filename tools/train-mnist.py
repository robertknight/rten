"""
MNIST digit recognition training script.

This script trains a small CNN model on the MNIST dataset to recognize digits
and exports the resulting models to ONNX format. The resulting model is small
so it can be stored as a regular file in Git and inference runs quickly even
with a debug build of RTen.

This script was generated by ChatGPT and lightly tweaked.
"""

import argparse
import os
from dataclasses import dataclass

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms


# A ~100KB MNIST model. Gets ~95% accuracy on MNIST test set after 10 epochs.
class MNIST100KB(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(
            1, 32, kernel_size=3, padding=1, bias=True
        )  # 1*32*3*3 + 32 = 320
        self.conv2 = nn.Conv2d(
            32, 72, kernel_size=3, padding=1, bias=True
        )  # 32*72*3*3 + 72 = 20,808
        self.pw = nn.Conv2d(72, 64, kernel_size=1, bias=True)  # 72*64 + 64 = 4,672
        self.fc = nn.Linear(64, 10, bias=True)  # 64*10 + 10 = 650
        # Total â‰ˆ 320 + 20,808 + 4,672 + 650 = 26,450 params

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2)  # 28x28 -> 14x14
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2)  # 14x14 -> 7x7
        x = F.relu(self.pw(x))
        x = F.adaptive_avg_pool2d(x, (1, 1))  # -> N x 64 x 1 x 1
        x = torch.flatten(x, 1)  # -> N x 64
        x = self.fc(x)  # -> N x 10
        return x


@dataclass
class Config:
    data_dir: str
    batch_size: int
    epochs: int
    lr: float
    weight_decay: float
    onnx_path: str
    opset: int
    seed: int
    num_workers: int
    device: str


def set_seed(seed: int):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    # For determinism (slight speed trade-offs)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def get_dataloaders(data_dir: str, batch_size: int, num_workers: int):
    transform = transforms.Compose(
        [
            transforms.ToTensor(),
            # Standard MNIST normalization
            transforms.Normalize((0.1307,), (0.3081,)),
        ]
    )
    train_ds = datasets.MNIST(
        root=data_dir, train=True, download=True, transform=transform
    )
    test_ds = datasets.MNIST(
        root=data_dir, train=False, download=True, transform=transform
    )

    train_loader = DataLoader(
        train_ds,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
    )
    test_loader = DataLoader(
        test_ds,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
    )
    return train_loader, test_loader


def train_one_epoch(model, loader, optimizer, device):
    model.train()
    total, correct, running_loss = 0, 0, 0.0
    for images, labels in loader:
        images = images.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)

        optimizer.zero_grad(set_to_none=True)
        logits = model(images)
        loss = F.cross_entropy(logits, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * labels.size(0)
        preds = logits.argmax(dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    return running_loss / total, correct / total


@torch.inference_mode()
def evaluate(model, loader, device):
    model.eval()
    total, correct, running_loss = 0, 0, 0.0
    for images, labels in loader:
        images = images.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)
        logits = model(images)
        loss = F.cross_entropy(logits, labels)
        running_loss += loss.item() * labels.size(0)
        preds = logits.argmax(dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)
    return running_loss / total, correct / total


def export_onnx(model, onnx_path: str, opset: int, device: str):
    model.eval()
    dummy = torch.randn(1, 1, 28, 28, device=device)  # NCHW
    input_names = ["input"]
    output_names = ["logits"]
    dynamic_axes = {
        "input": {0: "batch"},
        "logits": {0: "batch"},
    }

    # Move to CPU for a universally portable graph (optional but typical)
    model_cpu = model.to("cpu").eval()
    dummy_cpu = dummy.to("cpu")

    torch.onnx.export(
        model_cpu,
        dummy_cpu,
        onnx_path,
        export_params=True,
        opset_version=opset,
        do_constant_folding=True,
        input_names=input_names,
        output_names=output_names,
        dynamic_axes=dynamic_axes,
        dynamo=True,
        external_data=False,
    )
    print(f"[OK] Exported ONNX to: {onnx_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Train a tiny MNIST CNN (optional) and export to ONNX."
    )
    parser.add_argument(
        "--data",
        default="./data",
        help="Dataset directory (will be created if missing).",
    )
    parser.add_argument("--batch-size", type=int, default=128)
    parser.add_argument(
        "--epochs", type=int, default=5, help="Training epochs; use 0 to skip training."
    )
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--weight-decay", type=float, default=0.0)
    parser.add_argument(
        "--onnx", type=str, default="mnist.onnx", help="Output ONNX path."
    )
    parser.add_argument("--opset", type=int, default=17, help="ONNX opset version")
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--workers", type=int, default=2)
    parser.add_argument(
        "--device",
        type=str,
        default="cuda"
        if torch.cuda.is_available()
        else "mps"
        if torch.mps.is_available()
        else "cpu",
        choices=["cpu", "cuda", "mps"],
        help="Computation device.",
    )
    args = parser.parse_args()

    cfg = Config(
        data_dir=args.data,
        batch_size=args.batch_size,
        epochs=args.epochs,
        lr=args.lr,
        weight_decay=args.weight_decay,
        onnx_path=args.onnx,
        opset=args.opset,
        seed=args.seed,
        num_workers=args.workers,
        device=args.device,
    )

    # Create data dir if needed
    os.makedirs(cfg.data_dir, exist_ok=True)

    set_seed(cfg.seed)

    # Some backends may not be available; fall back gracefully
    if cfg.device == "cuda" and not torch.cuda.is_available():
        print("[Warn] CUDA not available; using CPU.")
        cfg.device = "cpu"
    if cfg.device == "mps" and not torch.backends.mps.is_available():
        print("[Warn] MPS not available; using CPU.")
        cfg.device = "cpu"

    device = torch.device(cfg.device)

    # Data
    train_loader, test_loader = get_dataloaders(
        cfg.data_dir, cfg.batch_size, cfg.num_workers
    )

    # Model / Optimizer
    model = MNIST100KB().to(device)
    optimizer = torch.optim.Adam(
        model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay
    )

    # Optional training
    if cfg.epochs > 0:
        print(f"Training for {cfg.epochs} epoch(s) on {cfg.device} ...")
        for epoch in range(1, cfg.epochs + 1):
            tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, device)
            te_loss, te_acc = evaluate(model, test_loader, device)
            print(
                f"Epoch {epoch:02d} | train loss {tr_loss:.4f} acc {tr_acc*100:.2f}% "
                f"| test loss {te_loss:.4f} acc {te_acc*100:.2f}%"
            )
    else:
        print("Skipping training (epochs=0). Exporting randomly initialized weights.")

    # Export to ONNX
    export_onnx(model, cfg.onnx_path, cfg.opset, cfg.device)


if __name__ == "__main__":
    main()
