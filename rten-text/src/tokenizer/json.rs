//! Types for the supported subset of the `tokenizer.json` pre-trained tokenizer
//! format.

use super::TokenId;
use serde_derive::Deserialize;

#[derive(Deserialize)]
pub(crate) struct AddedToken {
    pub content: String,
    pub id: TokenId,
}

#[derive(Deserialize)]
pub(crate) enum Pattern {
    Regex(String),
    String(String),
}

pub mod normalizers {
    use serde_derive::Deserialize;

    use super::{Normalizer, Pattern};

    #[derive(Deserialize)]
    pub(crate) struct Bert {
        pub lowercase: bool,
        pub strip_accents: Option<bool>,
    }

    #[derive(Deserialize)]
    pub(crate) struct Replace {
        pub pattern: Pattern,
        pub content: String,
    }

    #[derive(Deserialize)]
    pub(crate) struct Sequence {
        pub normalizers: Vec<Normalizer>,
    }
}

#[derive(Deserialize)]
#[serde(tag = "type")]
pub(crate) enum Normalizer {
    #[serde(rename = "BertNormalizer")]
    Bert(normalizers::Bert),
    Lowercase,
    #[serde(rename = "NFC")]
    Nfc,
    #[serde(rename = "NFD")]
    Nfd,
    #[serde(rename = "NFKC")]
    Nfkc,
    #[serde(rename = "NFKD")]
    Nfkd,
    Replace(normalizers::Replace),
    Sequence(normalizers::Sequence),
}

pub mod pre_tokenizers {
    use serde_derive::Deserialize;

    use super::{Pattern, PreTokenizer};

    #[derive(Deserialize)]
    pub(crate) struct ByteLevel {
        pub use_regex: bool,
    }

    #[derive(Deserialize)]
    pub(crate) struct Digits {
        pub individual_digits: bool,
    }

    #[derive(Deserialize)]
    pub(crate) struct Sequence {
        pub pretokenizers: Vec<PreTokenizer>,
    }

    #[derive(Deserialize)]
    pub(crate) enum SplitDelimiter {
        Removed,
        Isolated,
    }

    #[derive(Deserialize)]
    pub(crate) struct Split {
        pub pattern: Pattern,
        pub behavior: SplitDelimiter,
        pub invert: bool,
    }
}

/// Configuration for pre-tokenization.
///
/// See https://huggingface.co/docs/tokenizers/en/api/pre-tokenizers.
#[derive(Deserialize)]
#[serde(tag = "type")]
pub(crate) enum PreTokenizer {
    #[serde(rename = "BertPreTokenizer")]
    Bert,
    #[serde(rename = "ByteLevel")]
    ByteLevel(pre_tokenizers::ByteLevel),
    Digits(pre_tokenizers::Digits),
    Sequence(pre_tokenizers::Sequence),
    Split(pre_tokenizers::Split),
}

pub mod models {
    use std::borrow::Cow;
    use std::collections::HashMap;

    use rustc_hash::FxHashMap;
    use serde_derive::Deserialize;

    use crate::TokenId;

    #[derive(Deserialize)]
    pub(crate) struct WordPiece {
        /// Mapping from token text to token ID.
        pub vocab: HashMap<String, TokenId>,
    }

    #[derive(Debug, Deserialize)]
    #[serde(untagged)]
    pub(crate) enum MergeList<'a> {
        /// Pairs represented as a JSON array.
        #[serde(borrow)]
        Tuple(Vec<(Cow<'a, str>, Cow<'a, str>)>),
        /// Pairs represented as `<token_a> [SPACE] <token_b>`.
        Legacy(Vec<Cow<'a, str>>),
    }

    #[derive(Deserialize)]
    pub(crate) struct Bpe<'a> {
        /// Mapping from token text to token ID.
        pub vocab: FxHashMap<String, TokenId>,

        /// List of pairs of tokens to merge.
        #[serde(borrow)]
        pub merges: MergeList<'a>,

        /// A string which is implicitly appended to each substring after
        /// pre-tokenization before it is tokenized using BPE.
        ///
        /// This originated from CLIP's tokenizer.
        /// See https://github.com/openai/CLIP/blob/main/clip/simple_tokenizer.py.
        pub end_of_word_suffix: Option<String>,

        /// When encoding a string piece, look up the piece in the vocabulary
        /// before applying merge rules.
        pub ignore_merges: bool,
    }
}

#[derive(Deserialize)]
#[serde(tag = "type")]
pub(crate) enum Model<'a> {
    #[serde(borrow)]
    #[serde(rename = "BPE")]
    Bpe(models::Bpe<'a>),
    WordPiece(models::WordPiece),
}

/// Structure of the `tokenizers.json` files generated by Hugging Face
/// tokenizers [^1].
///
/// [^1]: https://github.com/huggingface/tokenizers
#[derive(Deserialize)]
pub(crate) struct Tokenizer<'a> {
    pub added_tokens: Option<Vec<AddedToken>>,
    pub normalizer: Option<Normalizer>,
    pub pre_tokenizer: Option<PreTokenizer>,
    #[serde(borrow)]
    pub model: Model<'a>,
}

/// Deserialize a `tokenizer.json` file.
pub fn from_json(json: &str) -> Result<Tokenizer<'_>, serde_json::Error> {
    serde_json::from_str(json)
}
